x <- seq(-10,10,length.out=20)
y <- ifelse(x > 0, 1, -1)
kernel <- function(x,y) return(kernel_gaussian(x, y, 1, 1))
K <- outer(x, x, Vectorize(function(x,y) kernel(x, y)))
get.laplace.approx <- function(y, K){
n <- length(y)
f <- rep(99, n); fnew <- rep(0, n)
ll <- get.log.lik()
while(mean((f - fnew)^2) > 1e-8){
f = fnew
W <- - diag(ll$d2log.lik.f2(y, f))
L <-  chol(diag(n) + sqrt(W) %*% K %*% sqrt(W))
b <-  W %*% f + ll$dlog.lik.f(y, f)
a <- b - solve(sqrt(W) %*% t(L), solve(t(L), sqrt(W) %*% K %*% b))
fnew = K %*% a
i=i+1
}
out <- list()
out$f.hat <- fnew
out$sigma.hat <- -diag(ll$d2log.lik.f2(y, out$f.hat))
out$sample <- function(n){
return(rmvnorm(n, out$f.hat, out$sigma.hat))
}
out$density <- function(f){
return(dmvnorm(f, out$f.hat, out$sigma.hat))
}
return(out)
}
laplace.approx <- get.laplace.approx(y, K)
knitr::opts_chunk$set(echo = TRUE)
library(mvtnorm)
get.gamma.prior <- function(alpha, beta){
out <- list()
out$sample <- function(){
return(rgamma(1, alpha, beta))
}
out$density <- function(x){
return(dgamma(x, alpha, beta))
}
return(out)
}
sigma.prior <- gamma.prior <- get.gamma.prior(5,5)
kernel_gaussian <- function(x, y, sigma, gamma){
return(sigma * exp(-0.5 / gamma^2 * sum((x - y)^2)))
}
get.log.lik <- function(){
out <- list()
out$log.lik <- function(y, f){
return(log(pnorm(y * f, 0, 1)))
}
out$dlog.lik.f <- function(y, f){
return(y * dnorm(f, 0, 1) / pnorm(y * f))
}
out$d2log.lik.f2 <- function(y, f){
return(as.vector(-dnorm(f)^2/pnorm(y*f)^2 - y * f * dnorm(f) / pnorm(y * f)))
}
return(out)
}
x <- seq(-10,10,length.out=20)
y <- ifelse(x > 0, 1, -1)
kernel <- function(x,y) return(kernel_gaussian(x, y, 1, 1))
K <- outer(x, x, Vectorize(function(x,y) kernel(x, y)))
get.laplace.approx <- function(y, K){
n <- length(y)
f <- rep(99, n); fnew <- rep(0, n)
ll <- get.log.lik()
while(mean((f - fnew)^2) > 1e-8){
f = fnew
W <- - diag(ll$d2log.lik.f2(y, f))
L <-  chol(diag(n) + sqrt(W) %*% K %*% sqrt(W))
b <-  W %*% f + ll$dlog.lik.f(y, f)
a <- b - solve(sqrt(W) %*% t(L), solve(t(L), sqrt(W) %*% K %*% b))
fnew = K %*% a
}
out <- list()
out$f.hat <- fnew
out$sigma.hat <- -diag(ll$d2log.lik.f2(y, out$f.hat))
out$sample <- function(n){
return(rmvnorm(n, out$f.hat, out$sigma.hat))
}
out$density <- function(f){
return(dmvnorm(f, out$f.hat, out$sigma.hat))
}
return(out)
}
laplace.approx <- get.laplace.approx(y, K)
laplace.approx$sample(10)
laplace.approx$density(rep(1,20))
laplace.approx$density(as.vector(laplace.approx$f.hat))
get.approx.marginal <- function(nimp, theta, laplace.approx){
ll <- get.log.lik()
kernel <- function(x,y) return(kernel_gaussian(x, y, theta[1], theta[2]))
K <- outer(x, x, Vectorize(function(x,y) kernel(x, y)))
gp.prior <- list()
gp.prior$density <- function(f){
return(dmvnorm(f, rep(0, length(f)), K))
}
# draw samples
f.samples <- laplace.approx$sample(nimp)
# sum
temp <- 0
for(i in 1:nimp){
f.temp <- f.samples[i,]
temp <- temp + pnorm(y * f.temp) * gp.prior$density(f.temp) / laplace.approx$density(f.temp)
}
return(temp/nimp)
}
get.approx.marginal(100, c(1,1), laplace.approx)
ell.ss.sample <- function(f, K){
n <- nrow(K)
ll <- get.log.lik()
z <- rmvnorm(1, rep(0, n), K)
u <- rexp(1)
eta <- prod(ll$log.lik(y, f)) - u
alpha <- runif(1, 0, 2*pi)
alpha.min <- alpha - 2*pi; alpha.max <- alpha
f.prop <- f * cos(alpha) + z * sin(alpha)
while(prod(ll$log.lik(y, f.prop)) < eta){
f.prop <- f * cos(alpha) + z * sin(alpha)
ifelse(alpha < 0, alpha.min <- 0, alpha.max <- 0)
alpha <- runif(1, alpha.min, alpha.max)
}
return(f.prop)
}
get.prop.distn <- function(sd){
out <- list()
out$sample <-  function(theta){
return(rnorm(1, theta, sd))
}
out$density = function(x, theta){
return(dnorm(x, theta, sd))
}
return(out)
}
prop.distn <- get.prop.distn(0.1)
algo.1 <- function(y, x, nsteps, nimp, init.theta, init.marginal.lik, init.f, prop.distn){
prior <- get.gamma.prior(5,5)
theta.samples <- matrix(NA, nrow=nsteps, ncol=length(init.theta))
f.samples <- matrix(NA, nrow=nsteps, ncol=length(y))
acceptance.ratio <- 0
kernel <- function(x,y) return(kernel_gaussian(x, y, init.theta[1], init.theta[2]))
K <- outer(x, x, Vectorize(function(x,y) kernel(x, y)))
laplace.approx <- get.laplace.approx(y, K)
p.tilde <- get.approx.marginal(nimp, init.theta, laplace.approx)
theta <- init.theta
for(i in 1:nsteps){
theta.prop <- c(prop.distn$sample(theta[1]), prop.distn$sample(theta[2]))
laplace.approx <- get.laplace.approx(y, K)
p.tilde.prop <- get.approx.marginal(nimp, theta.prop, laplace.approx)
acceptance.prob <- min(1, p.tilde.prop * prior$density(theta.prop) / p.tilde / prior$density(theta) * prop.distn$density(theta[1], theta.prop[1]) * prop.distn$density(theta[2], theta.prop[2]) / prop.distn$density(theta.prop[1], theta[1]) / prop.distn$density(theta.prop[1], theta[1]))
if(runif(1) < acceptance.prob){
theta <- theta.prop
p.tilde <- p.tilde.prop
kernel <- function(x,y) return(kernel_gaussian(x, y, theta[1], theta[2]))
K <- outer(x, x, Vectorize(function(x,y) kernel(x, y)))
acceptance.ratio <- acceptance.ratio + 1
}
theta.samples[i,] <- theta
f.samples[i,] <- f <- ell.ss.sample(f, K)
}
acceptance.ratio <- acceptance.ratio/nsteps
out <- list()
out$acceptance.ratio <- acceptance.ratio
out$theta.samples <- theta.samples
out$f.samples <- f.samples
return(out)
}
temp <- algo.1(y, x, 3, 100, c(1,1), rep(0,20), rep(0,20), get.prop.distn(0.01))
alpha=-1
ifelse(alpha < 0, alpha.min <- 0, alpha.max <- 0)
alpha.min
alpha.max
?ifelse
library(mvtnorm)
x = rmvnorm(500, c(1,1))
library(mvtnorm)
x = rmvnorm(500, c(1,1))
pd = data.frame(x=x, y=y)
library(mvtnorm)
x = rmvnorm(500, c(1,1))
y = rmvnorm(50, c(.5, .3), sigma = diag(0.05))
library(ggplot2)
pd = data.frame(x=x, y=y)
y = rmvnorm(50, c(.5, .3), sigma = diag(0.05, 2))
library(ggplot2)
pd = data.frame(x=x, y=y)
pd
ggplot(pd) + geom_point(aes(x=x.1, y=y.1))
x = rmvnorm(500, c(1,1))
ggplot(pd) + geom_point(aes(x=x.1, y=y.1))
?rmvnorm
set.seed(100)
x = rmvnorm(500, c(1,1))
ggplot(pd) + geom_point(aes(x=x.1, y=y.1))
pd = data.frame(x=x, y=y)
ggplot(pd) + geom_point(aes(x=x.1, y=y.1))
ggplot(pd) + geom_point(aes(x=x.1, y=x.1))
ggplot(pd) + geom_point(aes(x=x.1, y=x.2))
ggplot(pd) + geom_point(aes(x=x.1, y=x.2)) + geom_bin2d(aes(x=y.1, y=y.2))
ggplot(pd) +  geom_bin2d(aes(x=y.1, y=y.2)) + geom_point(aes(x=x.1, y=x.2))
ggplot(pd) +  geom_bin2d(aes(x=y.1, y=y.2)) + geom_point(aes(x=x.1, y=x.2), bins=70)
ggplot(pd) +  geom_bin2d(aes(x=y.1, y=y.2)) + geom_point(aes(x=x.1, y=x.2), bins=1000)
ggplot(pd) +  geom_bin2d(aes(x=y.1, y=y.2)) + geom_density2d(aes(x=x.1, y=x.2), bins=1000)
ggplot(pd) +  geom_bin2d(aes(x=y.1, y=y.2)) + geom_density2d(aes(x=x.1, y=x.2))
ggplot(pd) +  geom_density2d(aes(x=y.1, y=y.2)) + geom_point(aes(x=x.1, y=x.2))
ggplot(pd) +  stat_density2d(aes(x=y.1, y=y.2)) + geom_point(aes(x=x.1, y=x.2))
ggplot(pd) +  stat_density2d(aes(x=y.1, y=y.2, fill = ..level..)) + geom_point(aes(x=x.1, y=x.2))
ggplot(pd) +  stat_density_2d(aes(x=y.1, y=y.2, fill = ..level..)) + geom_point(aes(x=x.1, y=x.2))
ggplot(pd) +  stat_density_2d(aes(x=y.1, y=y.2), fill = ..level..) + geom_point(aes(x=x.1, y=x.2))
ggplot(pd) +  stat_density_2d(aes(x=y.1, y=y.2, fill = ..level..), geom="polygon") + geom_point(aes(x=x.1, y=x.2))
ggplot(pd) +  geom_point(aes(x=x.1, y=x.2))  + stat_density_2d(aes(x=y.1, y=y.2, fill = ..level..), geom="polygon")
ggplot(pd) +  geom_point(aes(x=x.1, y=x.2))  + stat_density_2d(aes(x=y.1, y=y.2, fill = ..level..),
geom="polygon", alpha = 0.5)
ggplot(pd) +  geom_point(aes(x=x.1, y=x.2))  + stat_density_2d(aes(x=y.1, y=y.2, fill = ..level..),
geom="polygon", alpha = 0.75)
ggplot(pd) +  geom_point(aes(x=x.1, y=x.2))  + stat_density_2d(aes(x=y.1, y=y.2, fill = ..level..),
geom="polygon", alpha = 0.75)  +
scale_fill_distiller(palette=4, direction=-1)
ggplot(pd) +  geom_point(aes(x=x.1, y=x.2))  + stat_density_2d(aes(x=y.1, y=y.2, fill = ..level..),
geom="polygon", alpha = 0.75)  +
scale_fill_distiller(palette=3, direction=-1)
ggplot(pd) +  geom_point(aes(x=x.1, y=x.2))  + stat_density_2d(aes(x=y.1, y=y.2, fill = ..level..),
geom="polygon", alpha = 0.75)  +
scale_fill_distiller(palette=3, direction=)
ggplot(pd) +  geom_point(aes(x=x.1, y=x.2))  + stat_density_2d(aes(x=y.1, y=y.2, fill = ..level..),
geom="polygon", alpha = 0.75)  +
scale_fill_distiller(palette=2)
ggplot(pd) +  geom_point(aes(x=x.1, y=x.2))  + stat_density_2d(aes(x=y.1, y=y.2, fill = ..level..),
geom="polygon", alpha = 0.75)  +
scale_fill_distiller(palette=1)
ggplot(pd) +  geom_point(aes(x=x.1, y=x.2))  + stat_density_2d(aes(x=y.1, y=y.2, fill = ..level..),
geom="polygon", alpha = 0.75)  +
scale_fill_distiller(palette=5)
ggplot(pd) +  geom_point(aes(x=x.1, y=x.2))  + stat_density_2d(aes(x=y.1, y=y.2, fill = ..level..),
geom="polygon", alpha = 0.75)  +
scale_fill_distiller(palette=6)
ggplot(pd) +  geom_point(aes(x=x.1, y=x.2))  + stat_density_2d(aes(x=y.1, y=y.2, fill = ..level..),
geom="polygon", alpha = 0.75)  +
scale_fill_distiller(palette="Spectral")
devtools::install_github("github.com/jakespiteri/GPclassification/
tree/master/code/package/gpc")
devtools::install_github("jakespiteri/GPclassification/
tree/master/code/package/gpc")
?devtools::install_github
devtools::install_git("http://github.com/jakespiteri/GPclassification/tree/master/code/package/gpc")
devtools::install_git("jakespiteri/GPclassification/", subdir="/code/package/gpc")
devtools::install_github("jakespiteri/GPclassification/", subdir="/code/package/gpc")
devtools::install_github("/jakespiteri/GPclassification/", subdir="/code/package/gpc")
devtools::install_github("/jakespiteri/GPclassification", subdir="/code/package/gpc")
devtools::install_github("jakespiteri/GPclassification", subdir="/code/package/gpc")
devtools::install_github("jakespiteri/GPclassification", subdir="code/package/gpc")
devtools::install_github("jakespiteri/GPclassification", subdir="code/package/gpc/")
devtools::install_github("jakespiteri/GPclassification", subdir="/code/package/gpc/")
?citet
library(kniknitcitations)
library(knitcitations)
?citet
bib <- read.bibtex("projectrefs.bib")
citet(bib)
citet(bib["storm"])
bibliography()
library(RefManageR)
library(gam)
library(mgcv)
gamSim
gamSim(1)
## Simulate some well correlated ordinal data with temporal structure.
# Use well established simulated data set in mgcv
library(mgcv)
dat <- gamSim(1)
y <- dat$y
x <- dat$f
mu <- dat$x2
## This is the temporal trend
# plot(mu,y)
# plot(mu,x)
# plot(x,y)
## Now we'll pretend the obs come from y while ensemble members come from x
## First make up some thresholds
tholds <- c(5, 9, 15)
## and a function to make a vector ordinal
get_discrete <- function(y,tholds){
n <- length(y)
result <- rep(1,n)
for(j in 1:length(tholds)) result[ y>tholds[j] ] <- 2 + (j-1)
result
}
characters = c(
"Dion", "Dion's Mum", "Dion's half bro", "Goth girl", "Silent gay", "Political Dreadlocks", "Headphones", "Drug dealer", "Princess", "Princess Daddy (Cop)", "Hookman", "Detention Teacher", "Mr Fade"
)
n = length(characters)
romana = characters[sample(characters, 9)]
romana
sample(characters, 9)
sample(characters, 8)
install.packages("googleLanguageR")
library(googleLanguageR)
gl_nlp("i am a god")
library(googleLanguageR)
a = gl_nlp("i am a god")
a$sentences
a$entities
??googleLanguageR
install.packages('blogdown')
blogdown::install_hugo()
setwd("~/Documents/Summer_Projects/kanyenet")
# Libraries
library(tidyverse)
## Read and load data
raw       = read.csv("nl_data/raw_nl_data.csv", stringsAsFactors = FALSE)
all_words = as.character(raw$words)
albums    = as.character(unique(raw$album))
bigdf     = matrix(0, 0, length(all_words)+1)
colnames(bigdf) = c("name", as.character(raw$song))
raw$types[1]
i=1
j=1
lyrics = all_words[i]
lyrics = substr(lyrics, 2, nchar(lyrics)-1)
song_name = as.character(raw$song[i])
song_pos = which(raw$song %in% song_name) + 1
words = stringr::str_split(lyrics, ",")
# Catch errors in splitting, manually subset
bad_split = which(!(substr(words[[1]][2:length(words[[1]])],1,2) == " '" |
substr(words[[1]][2:length(words[[1]])],1,2) ==' \"')) + 1
joined = paste0(words[[1]][bad_split-1], words[[1]][bad_split])
words[[1]][bad_split-1] = joined
words[[1]][bad_split] = NA
words[[1]] = words[[1]][!is.na(words[[1]])]
# Create smaller dataframe to append to larger one
df = as.data.frame(matrix(0, length(words[[1]]), length(all_words)+1), stringsAsFactors=FALSE)
colnames(df) = c("name", as.character(raw$song))
line = words[[1]][j]
split = strsplit(line, ":")
name = split[[1]][1]
name = substr(name, 3, nchar(name)-1)
x = factor(c("high", "low", "low"))
x2 = factor(c("a lot", "not so much", "other"))
model.matrix(x, x2)
x
model.matrix(cbind(x, x2))
model.matrix(~x + x2, cbind(x, x2))
model.matrix(~x + x2, data.frame(cbind(x, x2)))
data.frame(cbind(x, x2)))
data.frame(cbind(x, x2))
x2 = (c("a lot", "not so much", "other"))
x = (c("high", "low", "low"))
data.frame(cbind(x, x2))
model.matrix(~x + x2, data.frame(cbind(x, x2)))
nods
nodes
# Libraries
library(tidyverse)
## Read and load data
raw       = read.csv("nl_data/raw_nl_data.csv", stringsAsFactors = FALSE)
all_words = as.character(raw$words)
albums    = as.character(unique(raw$album))
bigdf     = matrix(0, 0, length(all_words)+1)
colnames(bigdf) = c("name", as.character(raw$song))
## Loop over word dictionaries, do string editing to convert to R dataframe
# Concatenate data frame into a larger dataframe
for(i in 1:length(all_words)){
lyrics = all_words[i]
lyrics = substr(lyrics, 2, nchar(lyrics)-1)
song_name = as.character(raw$song[i])
song_pos = which(raw$song %in% song_name) + 1
words = stringr::str_split(lyrics, ",")
# Catch errors in splitting, manually subset
bad_split = which(!(substr(words[[1]][2:length(words[[1]])],1,2) == " '" |
substr(words[[1]][2:length(words[[1]])],1,2) ==' \"')) + 1
joined = paste0(words[[1]][bad_split-1], words[[1]][bad_split])
words[[1]][bad_split-1] = joined
words[[1]][bad_split] = NA
words[[1]] = words[[1]][!is.na(words[[1]])]
# Create smaller dataframe to append to larger one
df = as.data.frame(matrix(0, length(words[[1]]), length(all_words)+1), stringsAsFactors=FALSE)
colnames(df) = c("name", as.character(raw$song))
for(j in 1:length(words[[1]])){
line = words[[1]][j]
split = strsplit(line, ":")
name = split[[1]][1]
name = substr(name, 3, nchar(name)-1)
value = split[[1]][2]
if(substr(value, nchar(value), nchar(value)) == "}"){
value = substr(value, 1, nchar(value)-1)
}
if(is.na(as.numeric(value))) cat("i = ", i, "name = ", name, ", value = ", value, "j = ", j, "\n")
value = as.numeric(value)
df[j, 1] = name
df[j, song_pos] = value
}
bigdf = rbind(bigdf, df)
}
## Convert sparse matrix into long format dataframe
library(reshape2)
bigdf2 = melt(bigdf)
bigdf2 = bigdf2[bigdf2$value!=0,]
bigdf2 = bigdf2[sample(1:nrow(bigdf2), nrow(bigdf2)),]
bigdf2 = bigdf2[order(bigdf2$value, decreasing = TRUE),]
# Remove plurals from words
remove_plurals = function(x){
ignores = c("jesus", "baby jesus", "glass", "mars",
"ass","parties", "glasses", "stress", "bitches")
if(x %in% ignores) return(x)
if(substr(x, nchar(x), nchar(x)) == "s"){
x = substr(x, 1, nchar(x)-1)
}
if(substr(x, nchar(x)-1, nchar(x)) == "es"){
x = substr(x, 1, nchar(x)-2)
}
return(x)
}
no_plurals = apply(as.matrix(bigdf2$name), 1, remove_plurals)
bigdf2$name = no_plurals
# love lockdown
# Censor bad words [content warning]
badwords = c("fuck", "shit", "nigga")
replacements = c("f***", "s***", "n****")
censor = function(x){
loc_censor = stringr::str_detect(x, badwords)
if(any(loc_censor)) {
if(sum(loc_censor) > 1){
y = x
for(j in 1:sum(loc_censor)){
y = stringr::str_replace(y, badwords[which(loc_censor)[j]],
replacements[which(loc_censor)[j]])
}
} else{
y = stringr::str_replace_all(x, badwords[loc_censor],
replacements[loc_censor])
}
return(y)
} else{
return(x)
}
}
censored_words = apply(as.matrix(bigdf2$name), 1, censor)
bigdf2$name = unlist(censored_words)
# Remove same word to same word mappings and small words (probable errors)
bigdf2 = bigdf2[!apply(bigdf2, 1, function(x) x[1] == x[2]),]
bigdf2 = bigdf2[apply(bigdf2, 1, function(x) nchar(x[1]) > 2 & nchar(x[2]) > 2),]
# Method of reducing the dataset - only include words that occur more than 4 times
bigdf2 = bigdf2[bigdf2$value > 4,]
## Two column dataframe mappings from word to word
songs = as.character(unique(bigdf2$variable))
bigdf3 = matrix(NA, 0, 2); colnames(bigdf3) = c("from", "to")
for(i in 1:length(songs)){
pos = which(bigdf2$variable %in% songs[i])
indf = bigdf2[pos,]
smalldf = matrix(NA, 0, 2)
for(j in 1:length(pos)){
indf2 = matrix(NA, nrow(indf), 2)
indf2[ ,1] = as.character(indf[j,1])
indf2[ ,2] = as.character(indf[ ,1])
smalldf = rbind(smalldf, indf2)
}
bigdf3 = rbind(bigdf3, smalldf)
}
# Libraries
library(igraph)
library(networkD3)
# Count number of unique pairs of mappings
bigdf3 = as.data.frame(bigdf3, stringsAsFactors=FALSE)
bigdf4 = data.frame(t(apply(bigdf3,1,sort)), stringsAsFactors = FALSE) %>% group_by_all %>% count()
colnames(bigdf4) = c("from", "to", "n")
## Get data in format for networkD3 forceNetwork plot
nodes = data.frame(name = unlist(bigdf2$name),
group = as.numeric(as.factor(bigdf2$variable)),
size = bigdf2$value, stringsAsFactors=FALSE)
nodes = nodes %>% group_by(name) %>%  summarise(size = sum(size))
nodes_i = 1:nrow(nodes) - 1
links = matrix(NA, nrow(bigdf4), 3)
for(i in 1:nrow(bigdf4)){
# from
from = as.character(bigdf4[i, "from"])
from_w = which(nodes$name %in% from)
links[i, 1] = nodes_i[from_w]
# to
to = as.character(bigdf4[i, "to"])
to_w = which(nodes$name %in% to)
links[i, 2] = nodes_i[to_w]
# count
links[i, 3] = as.numeric(bigdf4[i, "n"])
}
links = as.data.frame(links, stringsAsFactors = FALSE)
colnames(links) = c("source", "target", "value")
# write.csv(links, "nl_data/graph_links.csv")
# write.csv(nodes, "nl_data/graph_nodes.csv")
click_script = 'alert("Total count of \'" + (d.name) + "\': " + (d.size))'
# plot the network
fn = forceNetwork(links, as.data.frame(nodes),
Source = "source", Target = "target",
NodeID = "name", Group = 1, Nodesize="size",
Value = "value", linkDistance = JS("function(d){return d.value * 10}"),
zoom=TRUE, opacity=0.9, opacityNoHover = 0.3,
linkColour = "#c2c2c2", charge = -50,
clickAction = click_script,
colourScale = JS("d3.scaleOrdinal(d3.schemeCategory10);"),
fontSize = 16, fontFamily = "Calibri")
fn$x$nodes$size <- nodes$size
fn$x$nodes$name <- nodes$name
htmlwidgets::onRender(
fn,
'function(el, x) {
d3.selectAll(".node text").style("fill", "black");
}'
)
nodes
links
fn$x
fn$x$links$source
fn$x$links$target
